diff --git a/examples/train_offline_pixels_widowx.py b/examples/train_offline_pixels_widowx.py
index 9105dae..b6c2d45 100644
--- a/examples/train_offline_pixels_widowx.py
+++ b/examples/train_offline_pixels_widowx.py
@@ -1,104 +1,77 @@
 #! /usr/bin/env python
-import copy
-import sys
-import datetime
 import os
+
 os.environ["TPU_CHIPS_PER_HOST_BOUNDS"] = "1,1,1"
 os.environ["TPU_HOST_BOUNDS"] = "1,1,1"
-from jaxrl2.data.utils import get_task_id_mapping
-from jaxrl2.utils.general_utils import AttrDict
-from jaxrl2.agents import IQLLearner
-from jaxrl2.agents.sac.sac_learner import SACLearner
-from jaxrl2.agents.cql.pixel_cql_learner import PixelCQLLearner
-from jaxrl2.agents.pixel_bc.pixel_bc_learner import PixelBCLearner
-from jaxrl2.agents.sarsa import PixelSARSALearner
-from jaxrl2.agents.cql_parallel_overall.pixel_cql_learner import PixelCQLParallelLearner
 from jaxrl2.agents.cql_encodersep.pixel_cql_learner import PixelCQLLearnerEncoderSep
-from jaxrl2.agents.cql_encodersep_method.pixel_cql_learner import PixelCQLLearnerEncoderSepMethod
-from jaxrl2.agents.cql_encodersep_parallel.pixel_cql_learner import PixelCQLLearnerEncoderSepParallel
-from jaxrl2.agents.cql_encodersep_dueling.pixel_cql_learner import PixelCQLLearnerEncoderSepDueling
-from jaxrl2.wrappers.rescale_actions_wrapper import RescaleActions
-from jaxrl2.wrappers.prev_action_wrapper import PrevActionStack
-from jaxrl2.wrappers.state_wrapper import StateStack
 from jaxrl2.wrappers.dummy_env import DummyEnv
-from jaxrl2.data.replay_buffer import ReplayBuffer
-from jaxrl2.wrappers.reaching_reward_wrapper import ReachingReward
+from jaxrl2.data.eps_transition_dataset import EpisodicTransitionDataset
 from jaxrl2.utils.general_utils import add_batch_dim
-from jaxrl2.kitchen_play.combo_wrappers import Kitchen, ActionRepeat, NormalizeActions, TimeLimit
-from jaxrl2.data.kitchen_dataset import KitchenDataset
 import collections
-import jax.numpy as jnp
-from flax.training import checkpoints
-from flax.core.frozen_dict import freeze, unfreeze, FrozenDict
-import jax
-try:
-    from doodad.wrappers.easy_launch import sweep_function, save_doodad_config
-except:
-    print('Warning, doodad not found!')
-import time
-from gym.spaces import Dict
 
-import gym
 import numpy as np
-from tqdm import tqdm
-from absl import app, flags
-from gym.spaces import Box
-from ml_collections import config_flags
-
-from jaxrl2.data import MemoryEfficientReplayBuffer, MemoryEfficientReplayBufferParallel, NaiveReplayBuffer
-from jaxrl2.data.dataset import MixingReplayBuffer, MixingReplayBufferParallel, PropertyReplayBuffer
 
 from jaxrl2.utils.wandb_logger import WandBLogger, create_exp_name
-from examples.configs.dataset_config_sim import *
+from jaxrl2.data.widowx_configs import *
 
-import wandb
+from examples.train_utils_sim import (
+    offline_training_loop,
+)
 
-from jaxrl2.agents.cql.pixel_cql_learner import PixelCQLLearner
-from jaxrl2.agents import PixelIQLLearner
-from jaxrl2.wrappers import FrameStack, obs_latency
-from examples.train_utils_sim import offline_training_loop, trajwise_alternating_training_loop, stepwise_alternating_training_loop, load_buffer
-from jaxrl2.wrappers.reaching_reward_wrapper import compute_distance_reward
-from jaxrl2.evaluation import evaluate
-import argparse
-import imp
 
 def main(variant):
-    import jax        
-    variant.stochastic_evals=False
-    
-    kwargs = variant['train_kwargs']
-    if kwargs.pop('cosine_decay', False):
-        kwargs['decay_steps'] = variant.max_steps
+    import jax
+
+    variant.stochastic_evals = False
+
+    kwargs = variant["train_kwargs"]
+    if kwargs.pop("cosine_decay", False):
+        kwargs["decay_steps"] = variant.max_steps
 
     if variant.suffix:
-        expname = create_exp_name(variant.prefix, seed=variant.seed) + f"_{variant.suffix}"
+        expname = (
+            create_exp_name(variant.prefix, seed=variant.seed) + f"_{variant.suffix}"
+        )
     else:
         expname = create_exp_name(variant.prefix, seed=variant.seed)
-    outputdir = os.environ['EXP'] + '/jaxrl/' + expname
+    outputdir = os.environ["EXP"] + "/jaxrl/" + expname
     variant.outputdir = outputdir
-    print('writing to output dir ', outputdir)
+    print("writing to output dir ", outputdir)
+
+    group_name = variant.prefix + "_" + variant.launch_group_id
+    wandb_logger = WandBLogger(
+        variant.prefix != "",
+        variant,
+        variant.wandb_project,
+        experiment_id=expname,
+        output_dir=outputdir,
+        group_name=group_name,
+    )
 
-    group_name = variant.prefix + '_' + variant.launch_group_id
-    wandb_logger = WandBLogger(variant.prefix != '', variant, variant.wandb_project, experiment_id=expname, output_dir=outputdir, group_name=group_name)
-    
     env = DummyEnv()
 
     sample_obs = add_batch_dim(env.observation_space.sample())
     sample_action = add_batch_dim(env.action_space.sample())
-    print('sample obs shapes', [(k, v.shape) for k, v in sample_obs.items()])
-    print('sample action shapes', sample_action.shape)
-    
+    print("sample obs shapes", [(k, v.shape) for k, v in sample_obs.items()])
+    print("sample action shapes", sample_action.shape)
+
     agent = PixelCQLLearnerEncoderSep(variant.seed, sample_obs, sample_action, **kwargs)
-                                
-    if variant.restore_path != '':
-        agent.restore_checkpoint(variant.restore_path, reset_critic=variant.reset_critic, rescale_critic_last_layer_ratio=variant.rescale_critic_last_layer_ratio)
-    
+
+    if variant.restore_path != "":
+        agent.restore_checkpoint(
+            variant.restore_path,
+            reset_critic=variant.reset_critic,
+            rescale_critic_last_layer_ratio=variant.rescale_critic_last_layer_ratio,
+        )
+
     online_replay_buffer = None
     if not variant.online_from_scratch:
         if variant.rew_func_for_target_only:
-            curr_rft = variant.reward_func_type if hasattr(variant, 'reward_func_type') else 0
+            curr_rft = (
+                variant.reward_func_type if hasattr(variant, "reward_func_type") else 0
+            )
             variant.reward_func_type = 0
-        
+
         if variant.rew_func_for_target_only:
             variant.reward_func_type = curr_rft
 
@@ -106,265 +79,26 @@ def main(variant):
             print("setting nstep return off during offline phase")
             agent.online_bound_nstep_return = -1
         
-        kitchenset = KitchenDataset()
-        
-        replay_buffer = kitchenset
-        offline_training_loop(variant, agent, env, replay_buffer, None, wandb_logger, perform_control_evals=False)
-
-
-def load_mult_tasks(variant, train_tasks, num_traj_cutoff=None, traj_len_cutoff=None, split_pos_neg=False, split_by_traj=False):
-    if num_traj_cutoff is not None:
-        num_traj_cutoff = num_traj_cutoff if num_traj_cutoff >= 0 else None
-    if traj_len_cutoff is not None:
-        traj_len_cutoff = traj_len_cutoff if traj_len_cutoff >=0 else None
-    pos_buffer_size = neg_buffer_size = buffer_size = 0
-    all_trajs = []
-    for dataset_file in train_tasks:
-        task_size, trajs = load_buffer(dataset_file, variant, num_traj_cutoff=num_traj_cutoff, traj_len_cutoff=traj_len_cutoff, split_pos_neg=split_pos_neg, split_by_traj=split_by_traj)
-        if split_pos_neg:
-            pos_size, neg_size = task_size
-            pos_buffer_size += pos_size
-            neg_buffer_size += neg_size
-            buffer_size = (pos_buffer_size, neg_buffer_size)
+        config_type = variant.get('dataset', 'debug')
+        if config_type == 'debug':
+            dataset_paths = debug_config()
+        elif config_type == 'sorting':
+            dataset_paths = sorting_dataset()
+        elif config_type == 'pickplace':
+            dataset_paths = pickplace_dataset()
+        elif config_type == 'sorting_pickplace':
+            dataset_paths = sorting_pickplace_dataset()
         else:
-            buffer_size += task_size
-        all_trajs.extend(trajs)
-    return all_trajs, buffer_size
-    
-def is_positive_sample(traj, i, variant, task_name):
-    return i >= len(traj['observations']) - variant.num_final_reward_steps
-
-def is_positive_traj(traj):
-    return traj['rewards'][-1, 0] >= 1
-
-def is_positive_traj_timestep(traj, i):
-    return traj['rewards'][i, 0] >= 1
-
-def insert_data(variant, replay_buffer, trajs, run_test=False, task_id_mapping=None, split_pos_neg=False, split_by_traj=False):
-    if split_pos_neg:
-        assert isinstance(replay_buffer, MixingReplayBuffer)
-        pos_buffer, neg_buffer = replay_buffer.replay_buffers
-
-    if split_by_traj:
-        num_traj_pos = 0
-        num_traj_neg = 0
-
-    for traj_id, traj in enumerate(trajs):
-        if variant.frame_stack == 1:
-            action_queuesize = 1
-        else:
-            action_queuesize = variant.frame_stack - 1
-        prev_actions = collections.deque(maxlen=action_queuesize)
-        current_states = collections.deque(maxlen=variant.frame_stack)
-
-        for i in range(action_queuesize):
-            prev_action = np.zeros_like(traj['actions'][0])
-            if run_test:
-                prev_action[0] = -1
-            prev_actions.append(prev_action)
-
-        for i in range(variant.frame_stack):
-            state = traj['observations'][0]['state']
-            if run_test:
-                state[0] = 0
-            current_states.append(state)
-
-        if split_by_traj:
-            positive_traj = is_positive_traj(traj)
-            if positive_traj:
-                num_traj_pos += 1
-            else:
-                num_traj_neg += 1
-
-        # first process rewards, masks and mc_returns
-        masks = [] 
-        for i in range(len(traj['observations'])):
-            if variant.reward_type != 'final_one':
-                reward = compute_distance_reward(traj['observations'][i]['state'][:3], TARGET_POINT, variant.reward_type)
-                traj['rewards'][i] = reward
-                masks.append(1.0)
-            else:
-                reward = traj['rewards'][i]
-            
-                def def_rew_func(x):
-                    return x * variant.reward_scale + variant.reward_shift
-                    
-                if not hasattr(variant, 'reward_func_type') or variant.reward_func_type == 0:
-                    rew_func = def_rew_func
-                elif variant.reward_func_type == 1:
-                    def rew_func(rew):
-                        if rew < 0:
-                            return rew * 10 # buffers where terminate when place incorrectly
-                        if rew == 2:
-                            return 10
-                        else:
-                            return -1.0
-                elif variant.reward_func_type == 2:    
-                    def rew_func(rew):
-                        if rew == 0:
-                            return -10
-                        elif rew == 1:
-                            return -5
-                        elif rew == 2:
-                            return 100
-                        else:
-                            assert False
-                elif variant.reward_func_type == 3:    
-                    def rew_func(rew):
-                        if rew == 0:
-                            return -20
-                        elif rew == 1:
-                            return -10
-                        elif rew == 2:
-                            return -5
-                        elif rew == 3:
-                            return 10
-                        else:
-                            assert False
-                else:
-                    rew_func = def_rew_func
-                    
-                variant.reward_func = rew_func            
-                reward = rew_func(reward)
-                traj['rewards'][i] = reward
-                    
-                if traj['rewards'][i] == 10:
-                    masks.append(0.0)
-                else:
-                    masks.append(1.0)
-        # calculate reward to go
-        monte_carlo_return = calc_return_to_go(traj['rewards'].squeeze().tolist(), masks, variant.discount)
-        
-        if variant.get("online_bound_nstep_return", -1) > 1:
-            nstep_return = calc_nstep_return(variant.online_bound_nstep_return, traj['rewards'].squeeze().tolist(), masks, variant.discount)
-        else:
-            nstep_return = [0] * len(masks)
-
-
-# process obs, next_obs, actions and insert to buffer
-        for i in range(len(traj['observations'])):
-            if not split_by_traj:
-                is_positive = is_positive_sample(traj, i, variant, task_name=traj['task_description'])
-            else:
-                is_positive = is_positive_traj_timestep(traj, i)
-
-            obs = dict()
-            if not variant.from_states:
-                obs['pixels'] = traj['observations'][i]['image']
-                obs['pixels'] = obs['pixels'][..., np.newaxis]
-                if run_test:
-                    obs['pixels'][0, 0] = i
-            if variant.add_states:
-                obs['state'] = np.stack(current_states, axis=-1)
-            if variant.add_prev_actions:
-                obs['prev_action'] = np.stack(prev_actions, axis=-1)
-
-            action_i = traj['actions'][i]
-            if run_test:
-                action_i[0] = i
-            prev_actions.append(action_i)
-
-            current_state = traj['next_observations'][i]['state']
-            if run_test:
-                current_state[0] = i + 1
-            current_states.append(current_state)  # do not delay state, therefore use i instead of i
-
-            next_obs = dict()
-            if not variant.from_states:
-                next_obs['pixels'] = traj['next_observations'][i]['image']
-                next_obs['pixels'] = next_obs['pixels'][..., np.newaxis]
-                # if i == 0:
-                #     obs['pixels'] = np.tile(obs['pixels'], [1, 1, 1, variant.frame_stack])
-                if run_test:
-                    next_obs['pixels'][0, 0] = i + 1
-            if variant.add_states:
-                next_obs['state'] = np.stack(current_states, axis=-1)
-            if variant.add_prev_actions:
-                next_obs['prev_action'] = np.stack(prev_actions, axis=-1)
-
-            if task_id_mapping is not None:
-                if len(task_id_mapping.keys()) > 1:
-                    task_id = np.zeros((len(task_id_mapping.keys())))
-                    task_id[task_id_mapping[traj['task_description']]] = 1
-                    obs['task_id'] = task_id
-                    next_obs['task_id'] = task_id
-
-            if split_pos_neg:
-                if positive_traj:
-                    trajectory_id=pos_buffer._traj_counter
-                else:
-                    trajectory_id=neg_buffer._traj_counter
-            else:
-                trajectory_id=replay_buffer._traj_counter
-
-            insert_dict =  dict(observations=obs,
-                     actions=traj['actions'][i],
-                     next_actions=traj['actions'][i+1] if len(traj['actions']) > i+1 else traj['actions'][i],
-                     rewards=traj['rewards'][i],
-                     next_observations=next_obs,
-                     masks=masks[i],
-                     dones=bool(i == len(traj['observations']) - 1),
-                     trajectory_id=trajectory_id,
-                     mc_returns=monte_carlo_return[i],
-                     nstep_returns=nstep_return[i],
-                     is_offline = 1
-                     )
-
-            if split_pos_neg:
-                if positive_traj:
-                    pos_buffer.insert(insert_dict)
-                else:
-                    neg_buffer.insert(insert_dict)
-            else:
-                replay_buffer.insert(insert_dict)
-
-        if split_by_traj:
-            if positive_traj:
-                pos_buffer.increment_traj_counter()
-            else:
-                neg_buffer.increment_traj_counter()
-        else:
-            replay_buffer.increment_traj_counter()
-
-    if split_by_traj:
-        print('num traj pos', num_traj_pos)
-        print('num traj neg', num_traj_neg)
-
-
-RETURN_TO_GO_DICT = dict()
-
-def calc_return_to_go(rewards, masks, gamma):
-    global RETURN_TO_GO_DICT
-    rewards_str = str(rewards) + str(masks) + str(gamma)
-    if rewards_str in RETURN_TO_GO_DICT.keys():
-        reward_to_go = RETURN_TO_GO_DICT[rewards_str]
-    else:
-        reward_to_go = [0]*len(rewards)
-        prev_return = 0
-        for i in range(len(rewards)):
-            reward_to_go[-i-1] = rewards[-i-1] + gamma * prev_return * masks[-i-1]
-            prev_return = reward_to_go[-i-1]
-        RETURN_TO_GO_DICT[rewards_str] = reward_to_go
-    return reward_to_go
-
-NSTEP_RETURN_DICT = dict()
-def calc_nstep_return(n, rewards, masks, gamma):
-    global NSTEP_RETURN_DICT
-    rewards_str = str(rewards) + str(masks) + str(gamma)
-    if rewards_str in NSTEP_RETURN_DICT.keys():
-        nstep_return = NSTEP_RETURN_DICT[rewards_str]
-    else:
-        nstep_return = [0]*len(rewards)
-        prev_return = 0
-        terminal_counts=1
-        for i in range(len(rewards)):
-            if i < n + terminal_counts - 1:
-                nstep_return[-i-1] = rewards[-i-1] + gamma * prev_return * masks[-i-1]
-            else:
-                nstep_return[-i-1] = rewards[-i-1] + gamma * prev_return * masks[-i-1] - (gamma**n) * rewards[-i-1+n] * masks[-i-1]
-            prev_return = nstep_return[-i-1]
-            
-            if i!= 0 and masks[-i-1] == 0:
-                terminal_counts+=1
-        NSTEP_RETURN_DICT[rewards_str] = nstep_return
-    return nstep_return
\ No newline at end of file
+            raise ValueError(f"Unknown dataset type {config_type}")
+
+        replay_buffer = EpisodicTransitionDataset(dataset_paths)
+
+        offline_training_loop(
+            variant,
+            agent,
+            env,
+            replay_buffer,
+            None,
+            wandb_logger,
+            perform_control_evals=False,
+        )
\ No newline at end of file
diff --git a/jaxrl2/agents/__init__.py b/jaxrl2/agents/__init__.py
index ec836c7..3b8e53b 100644
--- a/jaxrl2/agents/__init__.py
+++ b/jaxrl2/agents/__init__.py
@@ -5,6 +5,5 @@ from jaxrl2.agents.pixel_bc import PixelBCLearner
 from jaxrl2.agents.pixel_iql import PixelIQLLearner
 from jaxrl2.agents.sac import SACLearner
 from jaxrl2.agents.pixel_cql import PixelCQLLearner ###===###
-from jaxrl2.agents.cql_encodersep_parallel import PixelCQLLearnerEncoderSepParallel ###---###
 from jaxrl2.agents.pixel_ddpm_bc import PixelDDPMBCLearner ###===###
 from jaxrl2.agents.pixel_idql.pixel_idql_learner import PixelIDQLLearner ###===###
diff --git a/jaxrl2/agents/cql_encodersep/pixel_cql_learner.py b/jaxrl2/agents/cql_encodersep/pixel_cql_learner.py
index adf8b6e..f64db2b 100755
--- a/jaxrl2/agents/cql_encodersep/pixel_cql_learner.py
+++ b/jaxrl2/agents/cql_encodersep/pixel_cql_learner.py
@@ -15,7 +15,6 @@ import jax
 import jax.numpy as jnp
 import optax
 import flax
-from flax import linen as nn
 from flax.core.frozen_dict import FrozenDict
 from flax.core import freeze, unfreeze
 from flax.training import train_state
@@ -31,25 +30,17 @@ from jaxrl2.utils.target_update import soft_target_update
 from jaxrl2.types import Params, PRNGKey
 
 from jaxrl2.agents.agent import Agent
-from jaxrl2.networks.learned_std_normal_policy import LearnedStdNormalPolicy, LearnedStdTanhNormalPolicy
+from jaxrl2.networks.learned_std_normal_policy import LearnedStdTanhNormalPolicy
 from jaxrl2.agents.drq.augmentations import batched_random_crop, color_transform
-from jaxrl2.agents.common import _unpack
-from jaxrl2.agents.drq.drq_learner import _share_encoder
 from jaxrl2.networks.encoders.networks import Encoder, PixelMultiplexer, PixelMultiplexerEncoder, PixelMultiplexerDecoder
 from jaxrl2.networks.encoders.impala_encoder import ImpalaEncoder
 from jaxrl2.networks.encoders.resnet_encoderv1 import ResNet18, ResNet34, ResNetSmall
 from jaxrl2.networks.encoders.resnet_encoderv2 import ResNetV2Encoder
-from jaxrl2.data.dataset import DatasetDict
-from jaxrl2.networks.normal_policy import NormalPolicy
-from jaxrl2.networks.values import StateActionEnsemble, StateValue
-from jaxrl2.networks.values.state_action_value import StateActionValue
-from jaxrl2.networks.values.state_value import StateValueEnsemble
+from jaxrl2.networks.values import StateActionEnsemble
 from jaxrl2.types import Params, PRNGKey
 from jaxrl2.utils.target_update import soft_target_update
 # from jaxrl_m.vision import bigvision_resnetv2 as resnet
 # from jaxrl_m.vision import encoders as encoders
-from icecream import ic
-import wandb
 
 import numpy as np
 from typing import Any
diff --git a/jaxrl2/data/dataset.py b/jaxrl2/data/dataset.py
index f26866f..2052efc 100644
--- a/jaxrl2/data/dataset.py
+++ b/jaxrl2/data/dataset.py
@@ -1,43 +1,54 @@
 from typing import Dict, Iterable, Optional, Tuple, Union
-
+import collections
+import jax
 import numpy as np
-from flax.core import frozen_dict
 from gym.utils import seeding
-
+import jax.numpy as jnp
 from jaxrl2.types import DataType
 
 DatasetDict = Dict[str, DataType]
+from flax.core import frozen_dict
 
+def concat_recursive(batches):
+    new_batch = {}
+    for k, v in batches[0].items():
+        if isinstance(v, frozen_dict.FrozenDict):
+            new_batch[k] = concat_recursive([batches[0][k], batches[1][k]])
+        else:
+            new_batch[k] = np.concatenate([b[k] for b in batches], 0)
+    return new_batch
 
-def _check_lengths(dataset_dict: DatasetDict, dataset_len: Optional[int] = None) -> int:
+def _check_lengths(dataset_dict: DatasetDict,
+                   dataset_len: Optional[int] = None) -> int:
     for v in dataset_dict.values():
         if isinstance(v, dict):
             dataset_len = dataset_len or _check_lengths(v, dataset_len)
         elif isinstance(v, np.ndarray):
             item_len = len(v)
             dataset_len = dataset_len or item_len
-            assert dataset_len == item_len, "Inconsistent item lengths in the dataset."
+            assert dataset_len == item_len, 'Inconsistent item lengths in the dataset.'
         else:
-            raise TypeError("Unsupported type.")
+            raise TypeError('Unsupported type.')
     return dataset_len
 
 
-def _subselect(dataset_dict: DatasetDict, index: np.ndarray) -> DatasetDict:
-    new_dataset_dict = {}
+def _split(dataset_dict: DatasetDict,
+           index: int) -> Tuple[DatasetDict, DatasetDict]:
+    train_dataset_dict, test_dataset_dict = {}, {}
     for k, v in dataset_dict.items():
         if isinstance(v, dict):
-            new_v = _subselect(v, index)
+            train_v, test_v = _split(v, index)
         elif isinstance(v, np.ndarray):
-            new_v = v[index]
+            train_v, test_v = v[:index], v[index:]
         else:
-            raise TypeError("Unsupported type.")
-        new_dataset_dict[k] = new_v
-    return new_dataset_dict
+            raise TypeError('Unsupported type.')
+        train_dataset_dict[k] = train_v
+        test_dataset_dict[k] = test_v
+    return train_dataset_dict, test_dataset_dict
 
 
-def _sample(
-    dataset_dict: Union[np.ndarray, DatasetDict], indx: np.ndarray
-) -> DatasetDict:
+def _sample(dataset_dict: Union[np.ndarray, DatasetDict],
+            indx: np.ndarray) -> DatasetDict:
     if isinstance(dataset_dict, np.ndarray):
         return dataset_dict[indx]
     elif isinstance(dataset_dict, dict):
@@ -50,6 +61,7 @@ def _sample(
 
 
 class Dataset(object):
+
     def __init__(self, dataset_dict: DatasetDict, seed: Optional[int] = None):
         self.dataset_dict = dataset_dict
         self.dataset_len = _check_lengths(dataset_dict)
@@ -73,14 +85,12 @@ class Dataset(object):
     def __len__(self) -> int:
         return self.dataset_len
 
-    def sample(
-        self,
-        batch_size: int,
-        keys: Optional[Iterable[str]] = None,
-        indx: Optional[np.ndarray] = None,
-    ) -> frozen_dict.FrozenDict:
+    def sample(self,
+               batch_size: int,
+               keys: Optional[Iterable[str]] = None,
+               indx: Optional[np.ndarray] = None) -> frozen_dict.FrozenDict:
         if indx is None:
-            if hasattr(self.np_random, "integers"):
+            if hasattr(self.np_random, 'integers'):
                 indx = self.np_random.integers(len(self), size=batch_size)
             else:
                 indx = self.np_random.randint(len(self), size=batch_size)
@@ -98,68 +108,264 @@ class Dataset(object):
 
         return frozen_dict.freeze(batch)
 
-    def split(self, ratio: float) -> Tuple["Dataset", "Dataset"]:
+    def split(self, ratio: float) -> Tuple['Dataset', 'Dataset']:
         assert 0 < ratio and ratio < 1
-        train_index = np.index_exp[: int(self.dataset_len * ratio)]
-        test_index = np.index_exp[int(self.dataset_len * ratio) :]
-
-        index = np.arange(len(self), dtype=np.int32)
-        self.np_random.shuffle(index)
-        train_index = index[: int(self.dataset_len * ratio)]
-        test_index = index[int(self.dataset_len * ratio) :]
-
-        train_dataset_dict = _subselect(self.dataset_dict, train_index)
-        test_dataset_dict = _subselect(self.dataset_dict, test_index)
+        index = int(self.dataset_len * ratio)
+        train_dataset_dict, test_dataset_dict = _split(self.dataset_dict,
+                                                       index)
         return Dataset(train_dataset_dict), Dataset(test_dataset_dict)
 
-    def _trajectory_boundaries_and_returns(self) -> Tuple[list, list, list]:
-        episode_starts = [0]
-        episode_ends = []
 
-        episode_return = 0
-        episode_returns = []
+class MixingReplayBuffer():
 
-        for i in range(len(self)):
-            episode_return += self.dataset_dict["rewards"][i]
+    def __init__(
+            self,
+            replay_buffers,
+            mixing_ratio
+    ):
 
-            if self.dataset_dict["dones"][i]:
-                episode_returns.append(episode_return)
-                episode_ends.append(i + 1)
-                if i + 1 < len(self):
-                    episode_starts.append(i + 1)
-                episode_return = 0.0
+        """
+        :param replay_buffers: sample from given replay buffer with specified probability
+        """
 
-        return episode_starts, episode_ends, episode_returns
+        self.replay_buffers = replay_buffers
+        self.mixing_ratio = mixing_ratio
+        assert len(replay_buffers) == 2
 
-    def filter(
-        self, percentile: Optional[float] = None, threshold: Optional[float] = None
-    ):
-        assert (percentile is None and threshold is not None) or (
-            percentile is not None and threshold is None
-        )
+    def sample(self,
+               batch_size: int,
+               keys: Optional[Iterable[str]] = None,
+               indx: Optional[np.ndarray] = None) -> frozen_dict.FrozenDict:
 
-        (
-            episode_starts,
-            episode_ends,
-            episode_returns,
-        ) = self._trajectory_boundaries_and_returns()
+        batches = []
+        if min(self.mixing_ratio, 1 - self.mixing_ratio) * batch_size < 1:
+            size_first = int(np.random.binomial(n=batch_size, p=self.mixing_ratio))
+        else:
+            size_first = int(np.floor(batch_size*self.mixing_ratio))
+        sub_batch_sizes = [size_first, batch_size - size_first]
+        for buf, sb in zip(self.replay_buffers, sub_batch_sizes):
+            if sb > 0:
+                batches.append(buf.sample(sb))
+        mixed_batch = concat_recursive(batches) if len(batches) > 1 else batches[0]
+        return frozen_dict.freeze(mixed_batch)
+
+    def set_mixing_ratio(self, mixing_ratio):
+        self.mixing_ratio = mixing_ratio
+
+    def seed(self, seed):
+        [b.seed(seed) for b in self.replay_buffers]
+
+    def __len__(self):
+        return sum([len(b) for b in self.replay_buffers])
+    
+    def length(self):
+        return [b.length() for b in self.replay_buffers]
+
+    def get_random_trajs(self, batch_size):
+        return [b.get_random_trajs(batch_size) for b in self.replay_buffers]
+
+
+    def get_iterator(self,
+                     batch_size: int,
+                     keys: Optional[Iterable[str]] = None,
+                     indx: Optional[np.ndarray] = None,
+                     queue_size: int = 2):
+        # See https://flax.readthedocs.io/en/latest/_modules/flax/jax_utils.html#prefetch_to_device
+        # queue_size = 2 should be ok for one GPU.
+
+        queue = collections.deque()
+
+        def enqueue(n):
+            for _ in range(n):
+                data = self.sample(batch_size, keys, indx)
+                queue.append(jax.device_put(data))
+
+        enqueue(queue_size)
+        while queue:
+            yield queue.popleft()
+            enqueue(1)
+    
+    def increment_traj_counter(self):
+        [b.increment_traj_counter() for b in self.replay_buffers]
+
+    def compute_action_stats(self):
+
+        action_stats_0 = self.replay_buffers[0].compute_action_stats()
+        action_stats_1 = self.replay_buffers[1].compute_action_stats()
+
+        ratio = self.mixing_ratio
+        actions_mean = ratio * action_stats_0['mean'] + (1 - ratio) * action_stats_1['mean']
+        actions_std = np.sqrt(ratio * action_stats_0['std'] ** 2 + (1 - ratio) * action_stats_1['std']** 2 + ratio * (1 - ratio) * (action_stats_0['mean'] - action_stats_1['mean']) ** 2)
+
+        return {'mean': actions_mean, 'std': actions_std}
+
+    def normalize_actions(self, action_stats):
+        # do not normalize gripper dimension (last dimension)
+        [b.normalize_actions(action_stats) for b in self.replay_buffers]
+        
+class MixingReplayBufferParallel():
+    
+    def __init__(
+            self,
+            replay_buffers,
+            mixing_ratio,
+            num_devices=len(jax.devices())
+    ):
 
-        if percentile is not None:
-            threshold = np.percentile(episode_returns, 100 - percentile)
+        """
+        :param replay_buffers: sample from given replay buffer with specified probability
+        """
 
-        bool_indx = np.full((len(self),), False, dtype=bool)
+        self.replay_buffers = replay_buffers
+        self.mixing_ratio = mixing_ratio
+        assert len(replay_buffers) == 2
+        self.num_devices=num_devices
 
-        for i in range(len(episode_returns)):
-            if episode_returns[i] >= threshold:
-                bool_indx[episode_starts[i] : episode_ends[i]] = True
+    def sample(self,
+               batch_size: int,
+               keys: Optional[Iterable[str]] = None,
+               indx: Optional[np.ndarray] = None) -> frozen_dict.FrozenDict:
 
-        self.dataset_dict = _subselect(self.dataset_dict, bool_indx)
+        batches = []
+        if min(self.mixing_ratio, 1 - self.mixing_ratio) * batch_size < 1:
+            size_first = int(np.random.binomial(n=batch_size, p=self.mixing_ratio))
+        else:
+            size_first = int(np.floor(batch_size*self.mixing_ratio))
+        sub_batch_sizes = [size_first, batch_size - size_first]
+        for buf, sb in zip(self.replay_buffers, sub_batch_sizes):
+            if sb > 0:
+                batches.append(buf.sample(sb))
+        mixed_batch = concat_recursive(batches) if len(batches) > 1 else batches[0]
+        return frozen_dict.freeze(mixed_batch)
+
+    def seed(self, seed):
+        [b.seed(seed) for b in self.replay_buffers]
+        
+    def __len__(self):
+        return sum([len(b) for b in self.replay_buffers])
+
+    def length(self):
+        return [b.length() for b in self.replay_buffers]
+
+    def get_random_trajs(self, batch_size):
+        return [b.get_random_trajs(batch_size) for b in self.replay_buffers]
+
+
+    def get_iterator(self,
+                     batch_size: int,
+                     keys: Optional[Iterable[str]] = None,
+                     indx: Optional[np.ndarray] = None,
+                     queue_size: int = 2):
+        # See https://flax.readthedocs.io/en/latest/_modules/flax/jax_utils.html#prefetch_to_device
+        # queue_size = 2 should be ok for one GPU.
+
+        queue = collections.deque()
+
+        def enqueue(n):
+            assert batch_size % self.num_devices == 0
+            effective_batch_size = batch_size // self.num_devices
+            for _ in range(n):
+                data = [self.sample(effective_batch_size, keys, indx) for _ in range(self.num_devices)]   
+                queue.append(jax.device_put_sharded(data, jax.devices()))
+
+        enqueue(queue_size)
+        while queue:
+            yield queue.popleft()
+            enqueue(1)
+    
+    def increment_traj_counter(self):
+        [b.increment_traj_counter() for b in self.replay_buffers]
+
+    def compute_action_stats(self):
+
+        action_stats_0 = self.replay_buffers[0].compute_action_stats()
+        action_stats_1 = self.replay_buffers[1].compute_action_stats()
+
+        ratio = self.mixing_ratio
+        actions_mean = ratio * action_stats_0['mean'] + (1 - ratio) * action_stats_1['mean']
+        actions_std = np.sqrt(ratio * action_stats_0['std'] ** 2 + (1 - ratio) * action_stats_1['std']** 2 + ratio * (1 - ratio) * (action_stats_0['mean'] - action_stats_1['mean']) ** 2)
+
+        return {'mean': actions_mean, 'std': actions_std}
+
+    def normalize_actions(self, action_stats):
+        # do not normalize gripper dimension (last dimension)
+        [b.normalize_actions(action_stats) for b in self.replay_buffers]
+
+
+
+"""
+Adds additional properties to the dataset.
+"""
+class PropertyReplayBuffer():
+    
+    def __init__(
+            self,
+            replay_buffer,
+            property_dict,
+    ):
 
-        self.dataset_len = _check_lengths(self.dataset_dict)
+        """
+        :param replay_buffers: sample from given replay buffer with specified probability
+        """
+        self.replay_buffer = replay_buffer
+        self.property_dict = property_dict
+
+    def sample(self, batch_size: int, keys: Optional[Iterable[str]] = None, indx: Optional[np.ndarray] = None):
+        batch = self.replay_buffer.sample(batch_size)
+        update_batch = {}
+        for name, value in self.property_dict.items():
+            if isinstance(value, np.ndarray):
+                update_batch[name] = jnp.array(value)
+                update_batch[name] = jnp.expand_dims(batch[name], axis=0).repeat(batch_size, axis=0)
+            elif isinstance(value, float):
+                update_batch[name] = jnp.full((batch_size, 1), value)
+            elif isinstance(value, int):
+                update_batch[name] = jnp.full((batch_size, 1), float(value))
+            else:
+                assert False, "Unsupported type"
+                
+            update_batch[name] = update_batch[name].astype(jnp.float32)
+            batch = batch.copy(add_or_replace=update_batch)
+        
+        return frozen_dict.freeze(batch)
 
-    def normalize_returns(self, scaling: float = 1000):
-        (_, _, episode_returns) = self._trajectory_boundaries_and_returns()
-        self.dataset_dict["rewards"] /= np.max(episode_returns) - np.min(
-            episode_returns
-        )
-        self.dataset_dict["rewards"] *= scaling
+    def seed(self, seed):
+        return self.replay_buffer.seed(seed)
+
+    def length(self):
+        return self.replay_buffer.length()
+
+    def get_random_trajs(self, batch_size):
+        return self.replay_buffer.get_random_trajs(batch_size)
+    
+    def get_iterator(self,
+                     batch_size: int,
+                     keys: Optional[Iterable[str]] = None,
+                     indx: Optional[np.ndarray] = None,
+                     queue_size: int = 2):
+        # See https://flax.readthedocs.io/en/latest/_modules/flax/jax_utils.html#prefetch_to_device
+        # queue_size = 2 should be ok for one GPU.
+
+        queue = collections.deque()
+
+        def enqueue(n):
+            for _ in range(n):
+                data = self.sample(batch_size, keys, indx)
+                queue.append(jax.device_put(data))
+
+        enqueue(queue_size)
+        while queue:
+            yield queue.popleft()
+            enqueue(1)
+    
+    def increment_traj_counter(self):
+        return self.replay_buffer.increment_traj_counter()
+
+    def compute_action_stats(self):
+        return self.replay_buffer.compute_action_stats()
+
+    def normalize_actions(self, action_stats):
+        return self.replay_buffer.normalize_actions(action_stats)
+    
+    def __len__(self) -> int:
+        return len(self.replay_buffer)
\ No newline at end of file
diff --git a/jaxrl2/data/eps_transition_dataset.py b/jaxrl2/data/eps_transition_dataset.py
index 37170d1..d30a464 100644
--- a/jaxrl2/data/eps_transition_dataset.py
+++ b/jaxrl2/data/eps_transition_dataset.py
@@ -1,6 +1,6 @@
-from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union
+from typing import Any, Iterable, Optional
 from collections import defaultdict, OrderedDict, deque
-from flax.core.frozen_dict import freeze, unfreeze, FrozenDict
+from flax.core.frozen_dict import FrozenDict
 import numpy as np
 import os
 import gc
@@ -9,6 +9,8 @@ from jaxrl2.data.dataset import Dataset
 import tqdm
 
 RETURN_TO_GO_DICT = dict()
+
+
 def calc_return_to_go(rewards, masks=None, gamma=0.99):
     if masks is None:
         masks = rewards
@@ -17,34 +19,42 @@ def calc_return_to_go(rewards, masks=None, gamma=0.99):
     if rewards_str in RETURN_TO_GO_DICT.keys():
         reward_to_go = RETURN_TO_GO_DICT[rewards_str]
     else:
-        reward_to_go = [0]*len(rewards)
-        prev_return = rewards[-1]/(1-gamma)
+        reward_to_go = [0] * len(rewards)
+        prev_return = rewards[-1] / (1 - gamma)
         for i in range(len(rewards)):
-            reward_to_go[-i-1] = rewards[-i-1] + gamma * prev_return * masks[-i-1]
-            prev_return = reward_to_go[-i-1]
+            reward_to_go[-i - 1] = rewards[-i - 1] + gamma * prev_return * masks[-i - 1]
+            prev_return = reward_to_go[-i - 1]
         RETURN_TO_GO_DICT[rewards_str] = reward_to_go
     return reward_to_go
 
+
 def is_dict_like(x):
-    return isinstance(x, dict) or isinstance(x, FrozenDict) or isinstance(x, defaultdict) or isinstance(x, OrderedDict)
-    
-    
+    return (
+        isinstance(x, dict)
+        or isinstance(x, FrozenDict)
+        or isinstance(x, defaultdict)
+        or isinstance(x, OrderedDict)
+    )
+
+
 def default_remapping():
     return {}
 
+
 def default_obs_remapping():
     mapping_obs = OrderedDict(
-        end_effector_pos = 'state', 
-        right_finger_qpos = 'state', 
-        right_finger_qvel = 'state', 
-        left_finger_qpos = 'state', 
-        left_finger_qvel = 'state', 
-        pixels = 'pixels', 
-        task_id = 'state'
+        end_effector_pos="state",
+        right_finger_qpos="state",
+        right_finger_qvel="state",
+        left_finger_qpos="state",
+        left_finger_qvel="state",
+        pixels="pixels",
+        task_id="state",
     )
-    
+
     return mapping_obs
 
+
 def remap_dict(d, remapping):
     new_d = {}
     for k in d.keys():
@@ -55,6 +65,7 @@ def remap_dict(d, remapping):
             new_d[remapped_k] = np.array(d[k])
     return new_d
 
+
 def npify_dict(d):
     for k in d.keys():
         if is_dict_like(d[k]):
@@ -63,16 +74,22 @@ def npify_dict(d):
             d[k] = np.array(d[k])
     return d
 
+
 def append_dicts(dict1, dict2):
-    assert set(dict1.keys()) == set(dict2.keys()), f"Keys don't match: {dict1.keys()} vs {dict2.keys()}"
+    assert set(dict1.keys()) == set(
+        dict2.keys()
+    ), f"Keys don't match: {dict1.keys()} vs {dict2.keys()}"
     for k in dict1.keys():
         if is_dict_like(dict1[k]):
             dict1[k] = append_dicts(dict1[k], dict2[k])
         else:
             dict1[k] = np.concatenate([dict1[k], dict2[k]], axis=0)
     return dict1
-    
-def append_nested_dict(concat_dict, addition, remapping={}, obs_remapping={}):
+
+
+def append_nested_dict(
+    concat_dict, addition, remapping={}, obs_remapping={}, add_framestack_dim=True
+):
     # convert addition to correct format
     new_format_dict = {}
     for k in addition.keys():
@@ -91,58 +108,84 @@ def append_nested_dict(concat_dict, addition, remapping={}, obs_remapping={}):
     new_format_dict = npify_dict(new_format_dict)
     # now remap and append
     new_format_dict = remap_dict(new_format_dict, remapping)
-    new_format_dict['observations'] = remap_dict(new_format_dict['observations'].item(), obs_remapping)
-    new_format_dict['next_observations'] = remap_dict(new_format_dict['next_observations'].item(), obs_remapping)
-    
+    new_format_dict["observations"] = remap_dict(
+        new_format_dict["observations"].item(), obs_remapping
+    )
+    new_format_dict["next_observations"] = remap_dict(
+        new_format_dict["next_observations"].item(), obs_remapping
+    )
+
+    if add_framestack_dim:
+        for k, v in new_format_dict["observations"].items():
+            new_format_dict["observations"][k] = v[..., None]
+        for k, v in new_format_dict["next_observations"].items():
+            new_format_dict["next_observations"][k] = v[..., None]
+
     if not concat_dict:
         return new_format_dict
     return append_dicts(concat_dict, new_format_dict)
 
-class EpisodicTransitionDataset(Dataset):    
-    def __init__(self, paths: Any, remapping=default_remapping(), obs_remapping=default_obs_remapping()):
+
+class EpisodicTransitionDataset(Dataset):
+    def __init__(
+        self,
+        paths: Any,
+        remapping=default_remapping(),
+        obs_remapping=default_obs_remapping(),
+        add_framestack_dim=True,
+    ):
         if isinstance(paths, str):
             paths = [paths]
         assert isinstance(paths, list)
-        
+
         self._paths = paths
         self.episodes = []
         self.episode_as_dict = None
 
         self.episodes_lens = []
         for path in paths:
-            assert os.path.exists(path), f'Path {path} does not exist'
-            print('Loading data from', path)
-            
+            assert os.path.exists(path), f"Path {path} does not exist"
+            print("Loading data from", path)
+
             data = np.load(path, allow_pickle=True).tolist()
-            
+
             self.episodes.extend(data)
-            
+
             succ = []
             for i in tqdm.tqdm(range(len(data))):
-                rews = np.array(data[i]['rewards'])
-                data[i]['mc_returns'] = calc_return_to_go(rews)
-                
+                rews = np.array(data[i]["rewards"])
+                data[i]["mc_returns"] = calc_return_to_go(rews)
+                data[i]["masks"] = 1 - np.array(data[i]["terminals"])
+
                 succ.append(rews.any())
                 self.episodes_lens.append(len(rews))
-                
-                self.episode_as_dict = append_nested_dict(self.episode_as_dict, data[i], remapping=remapping, obs_remapping=obs_remapping)
-            
-            print('Success rate:', np.mean(succ))
+
+                self.episode_as_dict = append_nested_dict(
+                    self.episode_as_dict,
+                    data[i],
+                    remapping=remapping,
+                    obs_remapping=obs_remapping,
+                    add_framestack_dim=add_framestack_dim,
+                )
+
+            print("Success rate:", np.mean(succ))
             gc.collect()
-        
+
         self.episodes_lens = np.array(self.episodes_lens)
         self.episodes = np.array(self.episodes)
         super().__init__(self.episode_as_dict)
-        
-        print('Total number of episodes:', len(self.episodes))
-        
-    def get_iterator(self,
-                    batch_size: int,
-                    keys: Optional[Iterable[str]] = None,
-                    indx: Optional[np.ndarray] = None,
-                    queue_size: int = 2):
-    # See https://flax.readthedocs.io/en/latest/_modules/flax/jax_utils.html#prefetch_to_device
-    # queue_size = 2 should be ok for one GPU.
+
+        print("Total number of episodes:", len(self.episodes))
+
+    def get_iterator(
+        self,
+        batch_size: int,
+        keys: Optional[Iterable[str]] = None,
+        indx: Optional[np.ndarray] = None,
+        queue_size: int = 2,
+    ):
+        # See https://flax.readthedocs.io/en/latest/_modules/flax/jax_utils.html#prefetch_to_device
+        # queue_size = 2 should be ok for one GPU.
 
         queue = deque()
 
@@ -155,13 +198,15 @@ class EpisodicTransitionDataset(Dataset):
         while queue:
             yield queue.popleft()
             enqueue(1)
-                    
+
 
 def main():
-    path = '/nfs/kun2/users/asap7772/binsort_bridge/test/actionnoise0.0_binnoise0.0_policysorting_sparse0/train/out.npy'
+    path = "/nfs/kun2/users/asap7772/binsort_bridge/test/actionnoise0.0_binnoise0.0_policysorting_sparse0/train/out.npy"
     dataset = EpisodicTransitionDataset(path)
     batch = dataset.sample(13)
+    print(jax.tree_util.tree_map(lambda x: x.shape, batch))
     breakpoint()
-    
-if __name__ == '__main__':
-    main()
\ No newline at end of file
+
+
+if __name__ == "__main__":
+    main()
